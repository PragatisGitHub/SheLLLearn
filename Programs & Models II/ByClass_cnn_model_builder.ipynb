{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional Neural Network for multi-class softmax classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dependencies\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt #For plots\n",
    "import pandas as pd #For Dataframes\n",
    "import numpy as np #For scientific computing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import Keras dependencies\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten, Dense\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_classes = 62\n",
    "epochs = 10\n",
    "img_rows, img_cols = 28, 28    # Input image dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Emnist_file_path = \"emnist/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping file for ascii to alphabet mapping\n",
    "\n",
    "mapping_file = Emnist_file_path + 'emnist-byclass-mapping.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '0', 1: '1', 2: '2', 3: '3', 4: '4', 5: '5', 6: '6', 7: '7', 8: '8', 9: '9', 10: 'A', 11: 'B', 12: 'C', 13: 'D', 14: 'E', 15: 'F', 16: 'G', 17: 'H', 18: 'I', 19: 'J', 20: 'K', 21: 'L', 22: 'M', 23: 'N', 24: 'O', 25: 'P', 26: 'Q', 27: 'R', 28: 'S', 29: 'T', 30: 'U', 31: 'V', 32: 'W', 33: 'X', 34: 'Y', 35: 'Z', 36: 'a', 37: 'b', 38: 'c', 39: 'd', 40: 'e', 41: 'f', 42: 'g', 43: 'h', 44: 'i', 45: 'j', 46: 'k', 47: 'l', 48: 'm', 49: 'n', 50: 'o', 51: 'p', 52: 'q', 53: 'r', 54: 's', 55: 't', 56: 'u', 57: 'v', 58: 'w', 59: 'x', 60: 'y', 61: 'z'}\n"
     ]
    }
   ],
   "source": [
    "# Reading the file into a list\n",
    "\n",
    "with open(mapping_file, 'r') as fin: \n",
    "    mapping = fin.readlines() \n",
    "    \n",
    "# Creating an ascii dictionary\n",
    "\n",
    "ascii_map = {} \n",
    "for line in mapping: \n",
    "    char_class = int(line.split()[0]) \n",
    "    letter = chr(int(line.split()[1])) \n",
    "    ascii_map[char_class] = letter\n",
    "    \n",
    "print(ascii_map)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_train = Emnist_file_path + 'emnist-byclass-train.csv'\n",
    "All_test = Emnist_file_path + 'emnist-byclass-test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_train_data = pd.read_csv(All_train)\n",
    "All_test_data = pd.read_csv(All_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "All_training_data = All_train_data.values\n",
    "All_testing_data = All_test_data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "(697931, 785)\n",
      "(116322, 785)\n"
     ]
    }
   ],
   "source": [
    "print(type(All_training_data))\n",
    "print(type(All_testing_data))\n",
    "print(All_training_data.shape)\n",
    "print(All_testing_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape :  (697931, 784)\n",
      "y_train shape :  (697931, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = All_training_data[:, 1:].astype('float32')\n",
    "y_train = All_training_data[:, 0:1]\n",
    "print('x_train shape : ', x_train.shape)\n",
    "print('y_train shape : ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test shape :  (116322, 784)\n",
      "y_test shape :  (116322, 1)\n"
     ]
    }
   ],
   "source": [
    "x_test = All_testing_data[:, 1:].astype('float32')\n",
    "y_test = All_testing_data[:, 0:1]\n",
    "print('x_test shape : ', x_test.shape)\n",
    "print('y_test shape : ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for conv2d\n",
    "x_train = np.array(list(map(lambda x : x.reshape(28, 28).transpose().reshape(28, 28, 1), x_train)))\n",
    "x_test = np.array(list(map(lambda x : x.reshape(28, 28).transpose().reshape(28, 28, 1), x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "x_train shape :  (697931, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(type(x_train))\n",
    "print('x_train shape : ', x_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "x_test shape :  (116322, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(type(x_test))\n",
    "print('x_test shape : ', x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data  90089  :  [6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print the i data \n",
    "i = 90089\n",
    "plt.imshow(x_test[i].reshape(28, 28),cmap='gray')\n",
    "plt.colorbar()\n",
    "print('data ', i, ' : ', y_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make it 0 based indices\n",
    "\n",
    "y_train = y_train - 1\n",
    "y_test = y_test - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale from 0 to 255\n",
    "\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert labels to categorical one-hot encoding\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the CNN Architecture\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu',input_shape=input_shape))\n",
    "# filters: Integer (Here 32), the dimensionality of the output space (i.e. the number of output filters in the convolution).\n",
    "# kernel_size: An integer or tuple/list of 2 integers, specifying the height and width of the 2D convolution window. \n",
    "# Can be a single integer to specify the same value for all spatial dimensions.\n",
    "model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the Model\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=keras.optimizers.adam(), metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 24, 24, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 12, 12, 64)        0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9216)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               4719104   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 62)                31806     \n",
      "=================================================================\n",
      "Total params: 5,032,382\n",
      "Trainable params: 5,032,382\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Summarize our model\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Training - Test accuracy: 2.0314%\n"
     ]
    }
   ],
   "source": [
    "# Calculate the Classification Accuracy on the Test Set (Before Training)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "accuracy = 100*score[1]\n",
    "print('Before Training - Test accuracy: %.4f%%' % accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpointer = ModelCheckpoint(filepath='emnist.byclass.cnn.model.best.hdf5',\n",
    "                               verbose=1, save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 558344 samples, validate on 139587 samples\n",
      "Epoch 1/10\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.39754, saving model to emnist.byclass.cnn.model.best.hdf5\n",
      "Epoch 2/10\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.39754 to 0.37777, saving model to emnist.byclass.cnn.model.best.hdf5\n",
      "Epoch 3/10\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.37777 to 0.36213, saving model to emnist.byclass.cnn.model.best.hdf5\n",
      "Epoch 4/10\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.36213 to 0.35812, saving model to emnist.byclass.cnn.model.best.hdf5\n",
      "Epoch 5/10\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.35812 to 0.35323, saving model to emnist.byclass.cnn.model.best.hdf5\n",
      "Epoch 6/10\n"
     ]
    }
   ],
   "source": [
    "# Train the model, iterating on the data in batches of 256 samples with epochs 10\n",
    "\n",
    "Trained_Model = model.fit(x_train, y_train, batch_size=batch_size, epochs=10,\n",
    "          validation_split=0.2, callbacks=[checkpointer],\n",
    "          verbose=3, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Model with the Best Classification Accuracy on the Validation Set\n",
    "\n",
    "model.load_weights('emnist.byclass.cnn.model.best.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the best model\n",
    "\n",
    "model.save('eminst_byclass_cnn_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate test accuracy\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "\n",
    "accuracy = 100*score[1]\n",
    "loss = 100*score[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test accuracy: %.4f%%' % accuracy)\n",
    "print('Test loss: %.4f%%' % loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(Trained_Model.history['acc'])\n",
    "plt.plot(Trained_Model.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(Trained_Model.history['loss'])\n",
    "plt.plot(Trained_Model.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "We can use our trained model to make predictions using model.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.expand_dims(x_train[3], axis=0)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the i data \n",
    "i = 3\n",
    "plt.imshow(x_test[i].reshape(28, 28),cmap='gray')\n",
    "plt.colorbar()\n",
    "print('data ', i, ' : ', y_test[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a prediction. The resulting class should match the digit\n",
    "print(f\"One-Hot-Encoded Prediction: {model.predict(test).round()}\")\n",
    "print(f\"Predicted class: {model.predict_classes(test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"uploads/LetterX.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "image_size = (28, 28)\n",
    "im = image.load_img(filepath, target_size=image_size, grayscale=True)\n",
    "im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.expand_dims(img, axis=0)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the image to a numpy array \n",
    "from keras.preprocessing.image import img_to_array\n",
    "image = img_to_array(im)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = np.expand_dims(image, axis=0)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the image pixels by 255 (or use a scaler from sklearn here)\n",
    "image /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten into a 1x28*28 array \n",
    "img = image.flatten().reshape(-1, 28*28)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img.reshape(28, 28), cmap=plt.cm.Greys)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Invert the pixel values to match the original data\n",
    "img = 1 - img\n",
    "plt.imshow(img.reshape(28, 28), cmap=plt.cm.Greys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "pred = model.predict_classes(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[0]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_letter = ascii_map[pred[0]+1] \n",
    "pred_letter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
