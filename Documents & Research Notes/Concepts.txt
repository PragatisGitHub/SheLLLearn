Why dropout?

https://www.quora.com/How-does-the-dropout-method-work-in-deep-learning-And-why-is-it-claimed-to-be-an-effective-trick-to-improve-your-network

Now coming to the drop-out technique, we simply turn-off some of the hidden units (with some probability) so that the hidden units don’t learn every redundant detail of instances in training set. For example, lets say we have 3 hidden layers in neural network and each hidden layer contains 4 units, now lets suppose you turn-off two hidden units in 1st layer, due to which the activations (from the *off* hidden units) are not propagated forward. Therefore, the units in next layers have to learn from the *less data* (the activations from previous layer which have not been turned-off). If you analyze this phenomena, it serves multiple purposes. First, the hidden units in subsequent layers don’t have access to *every detail* of input instance, so they might have to figure out on their own on *how to create a function for this data* efficiently by using *less knowledge of data*. It will increase the efficiency of hidden units to find patterns for unseen data. Secondly, when some of the hidden units are turned-off, it reduces the computations, so the training speed of neural nets will increase. Thirdly you will have a model that gives a *compact representation of your data* (underlying patterns) with almost same effectiveness as compared to a very complex model.